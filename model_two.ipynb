{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f232b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE is : cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import colorama\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'DEVICE is : {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4c9e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = './train/'\n",
    "valid_dir = './valid/'\n",
    "\n",
    "classes = os.listdir(train_dir)\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd436d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Counting Images in Category set :\n",
      "    Apple___Apple_scab: 2016 images\n",
      "    Apple___Black_rot: 1987 images\n",
      "    Apple___Cedar_apple_rust: 1760 images\n",
      "    Apple___healthy: 2008 images\n",
      "    Blueberry___healthy: 1816 images\n",
      "    Cherry_(including_sour)___Powdery_mildew: 1683 images\n",
      "    Cherry_(including_sour)___healthy: 1826 images\n",
      "    Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot: 1642 images\n",
      "    Corn_(maize)___Common_rust_: 1907 images\n",
      "    Corn_(maize)___Northern_Leaf_Blight: 1908 images\n",
      "    Corn_(maize)___healthy: 1859 images\n",
      "    Grape___Black_rot: 1888 images\n",
      "    Grape___Esca_(Black_Measles): 1920 images\n",
      "    Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 1722 images\n",
      "    Grape___healthy: 1692 images\n",
      "    Orange___Haunglongbing_(Citrus_greening): 2010 images\n",
      "    Peach___Bacterial_spot: 1838 images\n",
      "    Peach___healthy: 1728 images\n",
      "    Pepper,_bell___Bacterial_spot: 1913 images\n",
      "    Pepper,_bell___healthy: 1988 images\n",
      "    Potato___Early_blight: 1939 images\n",
      "    Potato___Late_blight: 1939 images\n",
      "    Potato___healthy: 1824 images\n",
      "    Raspberry___healthy: 1781 images\n",
      "    Soybean___healthy: 2022 images\n",
      "    Squash___Powdery_mildew: 1736 images\n",
      "    Strawberry___Leaf_scorch: 1774 images\n",
      "    Strawberry___healthy: 1824 images\n",
      "    Tomato___Bacterial_spot: 1702 images\n",
      "    Tomato___Early_blight: 1920 images\n",
      "    Tomato___Late_blight: 1851 images\n",
      "    Tomato___Leaf_Mold: 1882 images\n",
      "    Tomato___Septoria_leaf_spot: 1745 images\n",
      "    Tomato___Spider_mites Two-spotted_spider_mite: 1741 images\n",
      "    Tomato___Target_Spot: 1827 images\n",
      "    Tomato___Tomato_Yellow_Leaf_Curl_Virus: 1961 images\n",
      "    Tomato___Tomato_mosaic_virus: 1790 images\n",
      "    Tomato___healthy: 1926 images\n",
      "\n",
      "\n",
      "Total train Images : 70295\n"
     ]
    }
   ],
   "source": [
    "def image_count(data_dir):\n",
    "        print(f'\\nTrain Counting Images in Category set :')\n",
    "        total_img = 0   \n",
    "        # Iterate over each category (subfolder) in the split\n",
    "        \n",
    "        for sub_path in sorted(os.listdir(data_dir)):\n",
    "                sub_cat = os.path.join(data_dir,sub_path)\n",
    "                # Count the number of files, assuming they are all images\n",
    "                num_images = len([name for name in os.listdir(sub_cat) if os.path.isfile(\n",
    "                        os.path.join(sub_cat, name))])\n",
    "                print(f\"    {sub_path}: {num_images} images\")\n",
    "                total_img += num_images\n",
    "\n",
    "        print(f'\\n\\nTotal train Images : {total_img}')\n",
    "\n",
    "image_count(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d7b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting Images in Category set :\n",
      "    Apple___Apple_scab: 504 images\n",
      "    Apple___Black_rot: 497 images\n",
      "    Apple___Cedar_apple_rust: 440 images\n",
      "    Apple___healthy: 502 images\n",
      "    Blueberry___healthy: 454 images\n",
      "    Cherry_(including_sour)___Powdery_mildew: 421 images\n",
      "    Cherry_(including_sour)___healthy: 456 images\n",
      "    Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot: 410 images\n",
      "    Corn_(maize)___Common_rust_: 477 images\n",
      "    Corn_(maize)___Northern_Leaf_Blight: 477 images\n",
      "    Corn_(maize)___healthy: 465 images\n",
      "    Grape___Black_rot: 472 images\n",
      "    Grape___Esca_(Black_Measles): 480 images\n",
      "    Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 430 images\n",
      "    Grape___healthy: 423 images\n",
      "    Orange___Haunglongbing_(Citrus_greening): 503 images\n",
      "    Peach___Bacterial_spot: 459 images\n",
      "    Peach___healthy: 432 images\n",
      "    Pepper,_bell___Bacterial_spot: 478 images\n",
      "    Pepper,_bell___healthy: 497 images\n",
      "    Potato___Early_blight: 485 images\n",
      "    Potato___Late_blight: 485 images\n",
      "    Potato___healthy: 456 images\n",
      "    Raspberry___healthy: 445 images\n",
      "    Soybean___healthy: 505 images\n",
      "    Squash___Powdery_mildew: 434 images\n",
      "    Strawberry___Leaf_scorch: 444 images\n",
      "    Strawberry___healthy: 456 images\n",
      "    Tomato___Bacterial_spot: 425 images\n",
      "    Tomato___Early_blight: 480 images\n",
      "    Tomato___Late_blight: 463 images\n",
      "    Tomato___Leaf_Mold: 470 images\n",
      "    Tomato___Septoria_leaf_spot: 436 images\n",
      "    Tomato___Spider_mites Two-spotted_spider_mite: 435 images\n",
      "    Tomato___Target_Spot: 457 images\n",
      "    Tomato___Tomato_Yellow_Leaf_Curl_Virus: 490 images\n",
      "    Tomato___Tomato_mosaic_virus: 448 images\n",
      "    Tomato___healthy: 481 images\n",
      "\n",
      "\n",
      "Total Valid Images : 17572\n"
     ]
    }
   ],
   "source": [
    "def image_count(data_dir):\n",
    "        print(f'\\nCounting Images in Category set :')\n",
    "        total_img = 0\n",
    "        # Iterate over each category (subfolder) in the split\n",
    "        \n",
    "        for sub_path in sorted(os.listdir(data_dir)):\n",
    "            sub_cat = os.path.join(data_dir,sub_path)\n",
    "            # Count the number of files, assuming they are all images\n",
    "            num_images = len([name for name in os.listdir(sub_cat) if os.path.isfile(\n",
    "                    os.path.join(sub_cat, name))])\n",
    "            print(f\"    {sub_path}: {num_images} images\")\n",
    "            total_img += num_images\n",
    "\n",
    "        print(f'\\n\\nTotal Valid Images : {total_img}')\n",
    "image_count(valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae336e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  # Normalize the data to [-1, 1]\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  # Normalize the data to [-1, 1]\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  # Normalize the data to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c40dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "valid = datasets.ImageFolder(root=valid_dir, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559324b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 70295\n",
       "    Root location: ./train/\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       "           )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27aef7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec443ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128]) 0\n"
     ]
    }
   ],
   "source": [
    "img, label = train[0]\n",
    "print(img.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26abb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_image(image, label):\n",
    "#     print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n",
    "#     plt.imshow(image.permute(1, 2, 0))\n",
    "    \n",
    "    \n",
    "# image_list = [0, 3000, 5000, 8000, 12000, 15000, 60000, 70000]\n",
    "    \n",
    "# chs = 0\n",
    "# for img in image_list:\n",
    "#     chs += 1\n",
    "#     plt.subplot(2,4,chs)\n",
    "#     print(Fore.GREEN)\n",
    "#     plt.tight_layout()\n",
    "#     plt.xlabel(img,fontsize=10)\n",
    "#     plt.title(train[img][1])\n",
    "#     show_image(*train[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c961e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for moving data to device (CPU or GPU)\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ab6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading in the device (GPU if available else CPU)\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dataloader, device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dataloader:\n",
    "            yield to_device(b, self.device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b15fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53bba1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving data into GPU, WrappedDataLoader\n",
    "train_dataloader = DeviceDataLoader(train_dataloader, DEVICE)\n",
    "valid_dataloader = DeviceDataLoader(valid_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f17ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for calculating the accuracy\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22460ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        #images, labels = images.to(DEVICE), labels.to(DEVICE) # move to GPU\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        #images, labels = images.to(DEVICE), labels.to(DEVICE) # move to GPU\n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2baed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution block with BatchNormalization\n",
    "def ConvBlock(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "             nn.BatchNorm2d(out_channels),\n",
    "             nn.ReLU(inplace=True)]\n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(4))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "942a37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet architecture \n",
    "class CNN_NeuralNet(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_diseases):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, 64)\n",
    "        self.conv2 = ConvBlock(64, 128, pool=True) \n",
    "        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n",
    "        \n",
    "        self.conv3 = ConvBlock(128, 256, pool=True) \n",
    "        self.conv4 = ConvBlock(256, 512, pool=True)\n",
    "        #self.conv5 = ConvBlock(256, 256, pool=True)\n",
    "        #self.conv6 = ConvBlock(256, 512, pool=True)\n",
    "        #self.conv7 = ConvBlock(512, 512, pool=True)\n",
    "        \n",
    "        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n",
    "\n",
    "        # self.classifier = nn.Sequential(nn.MaxPool2d(4),\n",
    "        #                                nn.Flatten(),\n",
    "        #                                nn.Linear(512, num_diseases))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),  # Safe replacement\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, num_diseases)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x): # x is the loaded batch\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        #out = self.conv5(out)\n",
    "        #out = self.conv6(out)\n",
    "        #out = self.conv7(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53df18bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_NeuralNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=512, out_features=38, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining the model and moving it to the GPU\n",
    "# 3 is number of channels RGB, len(train.classes()) is number of diseases.\n",
    "model = to_device(CNN_NeuralNet(3, len(train.classes)), DEVICE) \n",
    "#model = model.to(DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce6a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f729727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    if optimizer is None:\n",
    "        raise ValueError(\"Optimizer is not defined.\")\n",
    "    if not optimizer.param_groups:\n",
    "        raise ValueError(\"Optimizer has no param groups.\")\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0670b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# history = [evaluate(model, valid_dataloader)]\n",
    "# history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d1565",
   "metadata": {},
   "source": [
    "### Hyperparameters Function:\n",
    "\n",
    "Now it's time to create a function that get epochs, learning rate, train and validation loader and optim function..\n",
    "\n",
    "Clear GPU memory after PyTorch model training without restarting kernel with torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b857847",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,\n",
    "                grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []  #For collecting the results\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # scheduler for one cycle learniing rate\n",
    "    #Sets the learning rate of each parameter group according to the 1cycle learning rate policy. \n",
    "    #The 1cycle policy anneals the learning rate from an initial learning rate to some \n",
    "    #maximum learning rate and then from that maximum learning rate to some minimum learning rate\n",
    "    #much lower than the initial learning rate. \n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr,\n",
    "                                                epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # gradient clipping\n",
    "            #Clip the gradients of an iterable of parameters at specified value.\n",
    "            #All from pytorch documantation.\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "                \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # recording and updating learning rates\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "             # validation\n",
    "        \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        \n",
    "    torch.save(model.state_dict(), 'C_Model.pth')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d39ae",
   "metadata": {},
   "source": [
    "### Training Model:\n",
    "\n",
    "Evaluate function added to history of model.\n",
    "\n",
    "Then we can define our hyperparameters like number of epochs, learning rate and ... .\n",
    "\n",
    "Now we can update history with fit_OneCycle function (Adding two function together). Attention to history = [] in the second function. Now we have model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c566d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 5\n",
    "lr_rate = 0.001\n",
    "grad_clip = 0.15\n",
    "weight_decay = 1e-4\n",
    "optimizer = torch.optim.Adam \n",
    "\n",
    "history = fit_OneCycle(num_epoch, lr_rate, model, train_dataloader, valid_dataloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
